{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26dba06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157358d6",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2f5ccca-2936-4389-9eae-c21016560e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_vault_path = r\"C:\\Users\\nonak\\Documents\\Thougts\"\n",
    "second_valt_path = r\"C:\\Users\\nonak\\Documents\\MyObsidianSetup\"\n",
    "\n",
    "daily_notes_path = r\"C:\\Users\\nonak\\Documents\\Thoughts\\Calendar\\DAILY\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c51556-af50-49ec-b5c6-7d3c942defac",
   "metadata": {},
   "source": [
    "### PropertyAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a97817-d014-40d3-8d09-756c13b8e6ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## extract data functions\n",
    "def get_file_metadata(file_path):\n",
    "    \"\"\"Get enhanced file metadata with relative path\"\"\"\n",
    "    stat = os.stat(file_path)\n",
    "    return {\n",
    "        'created': datetime.fromtimestamp(stat.st_ctime),\n",
    "        'modified': datetime.fromtimestamp(stat.st_mtime),\n",
    "        'size_kb': stat.st_size / 1024,\n",
    "        'rel_path': os.path.relpath(file_path, start=directory_to_scan)\n",
    "    }\n",
    "\n",
    "def analyze_frontmatter(directory):\n",
    "    \"\"\"Extract and analyze all frontmatter data with statistical insights\"\"\"\n",
    "    property_stats = defaultdict(Counter)\n",
    "    file_count = 0\n",
    "    dates = []\n",
    "    sizes = []\n",
    "    property_presence = Counter()\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_count += 1\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                if content.startswith(\"---\"):\n",
    "                    end_of_header = content.find(\"---\", 3)\n",
    "                    if end_of_header != -1:\n",
    "                        header = content[3:end_of_header].strip()\n",
    "                        try:\n",
    "                            metadata = get_file_metadata(file_path)\n",
    "                            dates.append(metadata['created'])\n",
    "                            sizes.append(metadata['size_kb'])\n",
    "                            \n",
    "                            data = yaml.safe_load(header) or {}\n",
    "                            for prop, value in data.items():\n",
    "                                property_presence[prop] += 1\n",
    "                                if isinstance(value, list):\n",
    "                                    for item in value:\n",
    "                                        property_stats[prop][str(item)] += 1\n",
    "                                else:\n",
    "                                    property_stats[prop][str(value)] += 1\n",
    "                        except yaml.YAMLError:\n",
    "                            pass\n",
    "    \n",
    "    # Calculate date statistics\n",
    "    date_stats = {}\n",
    "    if dates:\n",
    "        sorted_dates = sorted(dates)\n",
    "        date_stats = {\n",
    "            'oldest': min(dates),\n",
    "            'newest': max(dates),\n",
    "            'timespan': max(dates) - min(dates),\n",
    "            'median': sorted_dates[len(sorted_dates)//2],\n",
    "            'total_files': file_count,\n",
    "            'files_per_day': file_count / (max(dates) - min(dates)).days if len(dates) > 1 else 0\n",
    "        }\n",
    "    \n",
    "    # Calculate size statistics\n",
    "    size_stats = {}\n",
    "    if sizes:\n",
    "        size_stats = {\n",
    "            'total_mb': sum(sizes) / 1024,\n",
    "            'avg_kb': sum(sizes) / len(sizes),\n",
    "            'largest_kb': max(sizes),\n",
    "            'smallest_kb': min(sizes)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'property_stats': dict(property_stats),\n",
    "        'property_presence': property_presence,\n",
    "        'date_stats': date_stats,\n",
    "        'size_stats': size_stats,\n",
    "        'total_files': file_count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c05d21-de4f-4c64-8c84-5d3fdd2c43c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical report generated: C:\\Users\\nonak\\Documents\\Thoughts\\PropertyAnalysis.md\n"
     ]
    }
   ],
   "source": [
    "# Generate report and .md\n",
    "def generate_statistical_report(data, output_file=\"markdown_stats_report.md\"):\n",
    "    \"\"\"Generate a statistics-focused markdown report\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as md:\n",
    "        # Report Header\n",
    "        md.write(\"# Markdown Statistics Report\\n\\n\")\n",
    "        \n",
    "        md.write(f\"> **Analysis performed:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        md.write(f\"> **Total files analyzed:** {data['total_files']:,}\\n\\n\")\n",
    "        \n",
    "        if data['date_stats']:\n",
    "            stats = data['date_stats']\n",
    "            md.write(f\"> **Date range:** {stats['oldest'].strftime('%Y-%m-%d')} to {stats['newest'].strftime('%Y-%m-%d')}\\n\")\n",
    "            md.write(f\"> **Timespan:** {stats['timespan'].days} days\\n\")\n",
    "            md.write(f\"> **Average files per day:** {stats['files_per_day']:.2f}\\n\\n\")\n",
    "            md.write(f\"[[_index_notas|Acessar Index todas as notas]]\")\n",
    "        else:\n",
    "            md.write(\"*No date information available*\\n\")\n",
    "        \n",
    "        # # Size Analysis\n",
    "        # md.write(\"\\n## 📦 Size Analysis\\n\")\n",
    "        # if data['size_stats']:\n",
    "        #     stats = data['size_stats']\n",
    "        #     md.write(f\"> **Total content size:** {stats['total_mb']:.2f} MB\\n\")\n",
    "        #     md.write(f\"> **Average file size:** {stats['avg_kb']:.1f} KB\\n\")\n",
    "        #     md.write(f\"> **Largest file:** {stats['largest_kb']:.1f} KB\\n\")\n",
    "        #     md.write(f\"> **Smallest file:** {stats['smallest_kb']:.1f} KB\\n\")\n",
    "        #     md.write(f\"> **Size range:** {stats['largest_kb']/stats['smallest_kb']:.1f}x variation\\n\")\n",
    "        # else:\n",
    "        #     md.write(\"*No size information available*\\n\")\n",
    "        \n",
    "        # Property Prevalence\n",
    "        md.write(\"\\n\\n\\n## Property Counts\\n\")\n",
    "        md.write(\"| Property | Files | Coverage |\\n\")\n",
    "        md.write(\"|----------|-------|----------|\\n\")\n",
    "        for prop, count in data['property_presence'].most_common(15):\n",
    "            coverage = (count / data['total_files']) * 100\n",
    "            md.write(f\"| `{prop}` | {count} | {coverage:.1f}% |\\n\")\n",
    "        \n",
    "        # Property Value Analysis\n",
    "        md.write(\"\\n## By Property\\n\")\n",
    "        for prop, counter in data['property_stats'].items():\n",
    "            total = sum(counter.values())\n",
    "            unique = len(counter)\n",
    "            value_counts = f\"\\n> **Present in:** {data['property_presence'][prop]} files ({data['property_presence'][prop]/data['total_files']:.1%})\\n\"\n",
    "            md.write(f\"\\n##### `{prop}` {value_counts}\")\n",
    "                \n",
    "            # Show value distribution if not too large\n",
    "            if unique <= 15:\n",
    "                md.write(\"\\n**Value Distribution:**\\n\")\n",
    "                for value, count in counter.most_common():\n",
    "                    md.write(f\"- `{value}`: {count} ({count/total:.1%})\\n\")\n",
    "            else:\n",
    "                top_values = counter.most_common(5)\n",
    "                others = total - sum(count for _, count in top_values)\n",
    "                md.write(\"\\n> **Top Values:**\\n\")\n",
    "                for value, count in top_values:\n",
    "                    md.write(f\"- `{value}`: {count} ({count/total:.1%})\\n\")\n",
    "                md.write(f\"\\n> *Others ({unique-5} values)*: {others} ({others/total:.1%})\\n\")\n",
    "            \n",
    "            # Calculate Gini coefficient for inequality\n",
    "            # if unique > 1:\n",
    "            #     sorted_counts = sorted(counter.values())\n",
    "            #     n = len(sorted_counts)\n",
    "            #     gini = sum(abs(x-y) for x in sorted_counts for y in sorted_counts) / (2*n*sum(sorted_counts))\n",
    "            #     md.write(f\"> **Value inequality (Gini):** {gini:.3f} (0=equal, 1=unequal)\\n\")\n",
    "        \n",
    "        # Recommendations Section\n",
    "        # md.write(\"\\n## 🚀 Recommendations\\n\")\n",
    "        # md.write(\"### Based on your metadata patterns:\\n\")\n",
    "        \n",
    "        # # Property standardization opportunities\n",
    "        # common_props = [p for p, c in data['property_presence'].most_common(5) if c/data['total_files'] > 0.7]\n",
    "        # if common_props:\n",
    "        #     md.write(\"- These properties are nearly universal and could be enforced:\\n\")\n",
    "        #     for prop in common_props:\n",
    "        #         md.write(f\"  - `{prop}` (in {data['property_presence'][prop]/data['total_files']:.0%} of files)\\n\")\n",
    "        \n",
    "        # # Underused properties\n",
    "        # rare_props = [p for p, c in data['property_presence'].items() if 0 < c/data['total_files'] < 0.1]\n",
    "        # if rare_props:\n",
    "        #     md.write(\"\\n- These properties are rarely used and might need review:\\n\")\n",
    "        #     for prop in rare_props[:5]:\n",
    "        #         md.write(f\"  - `{prop}` (only {data['property_presence'][prop]} files)\\n\")\n",
    "        \n",
    "        # # High-value-diversity properties\n",
    "        # diverse_props = []\n",
    "        # for prop, counter in data['property_stats'].items():\n",
    "        #     unique = len(counter)\n",
    "        #     total = sum(counter.values())\n",
    "        #     if unique > 10 and total/unique < 3:\n",
    "        #         diverse_props.append((prop, unique))\n",
    "        \n",
    "        # if diverse_props:\n",
    "        #     md.write(\"\\n- These properties have many unique values with low repetition:\\n\")\n",
    "        #     for prop, unique in sorted(diverse_props, key=lambda x: x[1], reverse=True)[:3]:\n",
    "        #         md.write(f\"  - `{prop}` ({unique} unique values)\\n\")\n",
    "        #         values_sample = \", \".join(f\"`{v}`\" for v, _ in data['property_stats'][prop].most_common(3))\n",
    "        #         md.write(f\"    *Sample values:* {values_sample}\\n\")\n",
    "\n",
    "# Configuration\n",
    "directory_to_scan = r\"C:\\Users\\nonak\\Documents\\Thoughts\"\n",
    "output_file = r\"C:\\Users\\nonak\\Documents\\Thoughts\\PropertyAnalysis.md\"\n",
    "\n",
    "# Run analysis and generate report\n",
    "data = analyze_frontmatter(directory_to_scan)\n",
    "generate_statistical_report(data, output_file)\n",
    "print(f\"Statistical report generated: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1a848-07d8-4256-8bd5-1775b28739fa",
   "metadata": {},
   "source": [
    "### create a index note with all notes by folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb4662-ca5f-44e9-96dd-4bd41364ecf0",
   "metadata": {},
   "source": [
    "#### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "64a2dbb5-cacd-47bc-94cb-e58d5d842ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\Users\\nonak\\Documents\\Thoughts\\System\\index_notas.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "def contar_palavras(texto):\n",
    "    return len(texto.split())\n",
    "\n",
    "def get_file_metadata(file_path, directory_to_scan):\n",
    "    \"\"\"Get enhanced file metadata with relative path\"\"\"\n",
    "    stat = os.stat(file_path)\n",
    "    return {\n",
    "        'created': datetime.fromtimestamp(stat.st_ctime),\n",
    "        'modified': datetime.fromtimestamp(stat.st_mtime),\n",
    "        'size_kb': stat.st_size / 1024,\n",
    "        'rel_path': os.path.relpath(file_path, start=directory_to_scan)\n",
    "    }\n",
    "\n",
    "def analyze_frontmatter(directory):\n",
    "    \"\"\"Extract and analyze all frontmatter data with statistical insights\"\"\"\n",
    "    property_stats = defaultdict(Counter)\n",
    "    file_count = 0\n",
    "    dates = []\n",
    "    sizes = []\n",
    "    property_presence = Counter()\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_count += 1\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                if content.startswith(\"---\"):\n",
    "                    end_of_header = content.find(\"---\", 3)\n",
    "                    if end_of_header != -1:\n",
    "                        header = content[3:end_of_header].strip()\n",
    "                        try:\n",
    "                            metadata = get_file_metadata(file_path, directory)\n",
    "                            dates.append(metadata['created'])\n",
    "                            sizes.append(metadata['size_kb'])\n",
    "                            \n",
    "                            data = yaml.safe_load(header) or {}\n",
    "                            for prop, value in data.items():\n",
    "                                property_presence[prop] += 1\n",
    "                                if isinstance(value, list):\n",
    "                                    for item in value:\n",
    "                                        property_stats[prop][str(item)] += 1\n",
    "                                else:\n",
    "                                    property_stats[prop][str(value)] += 1\n",
    "                        except yaml.YAMLError:\n",
    "                            pass\n",
    "    \n",
    "    # Calculate date statistics\n",
    "    date_stats = {}\n",
    "    if dates:\n",
    "        sorted_dates = sorted(dates)\n",
    "        date_stats = {\n",
    "            'oldest': min(dates),\n",
    "            'newest': max(dates),\n",
    "            'timespan': max(dates) - min(dates),\n",
    "            'median': sorted_dates[len(sorted_dates)//2],\n",
    "            'total_files': file_count,\n",
    "            'files_per_day': file_count / (max(dates) - min(dates)).days if len(dates) > 1 else 0\n",
    "        }\n",
    "    \n",
    "    # Calculate size statistics\n",
    "    size_stats = {}\n",
    "    if sizes:\n",
    "        size_stats = {\n",
    "            'total_mb': sum(sizes) / 1024,\n",
    "            'avg_kb': sum(sizes) / len(sizes),\n",
    "            'largest_kb': max(sizes),\n",
    "            'smallest_kb': min(sizes)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'property_stats': dict(property_stats),\n",
    "        'property_presence': property_presence,\n",
    "        'date_stats': date_stats,\n",
    "        'size_stats': size_stats,\n",
    "        'total_files': file_count\n",
    "    }\n",
    "\n",
    "def listar_notas_markdown_organizadas(pasta_raiz):\n",
    "    notas_por_pasta = defaultdict(list)\n",
    "    contagem_palavras_por_pasta = defaultdict(int)\n",
    "    total_palavras_geral = 0\n",
    "    total_notas_geral = 0\n",
    "    datas_modificacao = []\n",
    "\n",
    "    for raiz, _, arquivos in os.walk(pasta_raiz):\n",
    "        caminho_relativo = os.path.relpath(raiz, pasta_raiz)\n",
    "        if caminho_relativo == '.':\n",
    "            continue  # Ignorar a raiz\n",
    "\n",
    "        for arquivo in arquivos:\n",
    "            if arquivo.endswith(\".md\") and arquivo != \"_index_notas.md\":\n",
    "                caminho_completo = os.path.join(raiz, arquivo)\n",
    "                nome_nota = os.path.splitext(arquivo)[0]\n",
    "                \n",
    "                with open(caminho_completo, 'r', encoding='utf-8') as f:\n",
    "                    conteudo = f.read()\n",
    "                    palavras = contar_palavras(conteudo)\n",
    "                    total_palavras_geral += palavras\n",
    "                    total_notas_geral += 1\n",
    "                    contagem_palavras_por_pasta[caminho_relativo] += palavras\n",
    "                \n",
    "                mod_time = os.path.getmtime(caminho_completo)\n",
    "                datas_modificacao.append(datetime.fromtimestamp(mod_time))\n",
    "                \n",
    "                notas_por_pasta[caminho_relativo].append((nome_nota, palavras))\n",
    "\n",
    "    return notas_por_pasta, contagem_palavras_por_pasta, total_palavras_geral, total_notas_geral, datas_modificacao\n",
    "\n",
    "def formatar_numero(num, decimal_places=1):\n",
    "    if isinstance(num, int):\n",
    "        return f\"{num:,}\".replace(\",\", \".\")\n",
    "    else:\n",
    "        return f\"{num:,.{decimal_places}f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "\n",
    "def salvar_em_markdown(notas_organizadas, contagem_por_pasta, total_geral, total_notas, datas_modificacao, frontmatter_data, caminho_saida):\n",
    "    with open(caminho_saida, 'w', encoding='utf-8') as f:\n",
    "        # Cabeçalho principal\n",
    "        f.write(f\"*Atualizado em {datetime.now().strftime('%Y/%m/%d %H:%M')}*\\n\\n\")\n",
    "        \n",
    "        # Seção de resumo estatístico\n",
    "        # f.write(\"# Informações Gerais\\n\\n\")\n",
    "        f.write(f\"> **Total de Palavras**: {formatar_numero(total_geral)} palavras\\n\")\n",
    "        f.write(f\"> **Total de Notas**: {formatar_numero(total_notas)} notas\\n\")\n",
    "        \n",
    "        media_palavras = total_geral / total_notas if total_notas > 0 else 0\n",
    "        f.write(f\"> **Média de palavras por nota**: {formatar_numero(media_palavras)} palavras\\n\")\n",
    "        f.write(f\"> **Pastas organizadas**: {formatar_numero(len(notas_organizadas))}\\n\")\n",
    "        \n",
    "        if contagem_por_pasta:\n",
    "            pasta_mais_palavras = max(contagem_por_pasta.items(), key=lambda x: x[1])\n",
    "            f.write(f\"> **Pasta mais densa**: `{pasta_mais_palavras[0]}` ({formatar_numero(pasta_mais_palavras[1])} palavras)\\n\")\n",
    "        \n",
    "        if notas_organizadas:\n",
    "            pasta_mais_notas = max(notas_organizadas.items(), key=lambda x: len(x[1]))\n",
    "            f.write(f\"> **Pasta com mais notas**: `{pasta_mais_notas[0]}` ({formatar_numero(len(pasta_mais_notas[1]))} notas)\\n\")\n",
    "        \n",
    "        # Add frontmatter statistics section\n",
    "        f.write(\"\\n---\\n\\n# 🟦 Resumo propriedades \\n\\n\")\n",
    "        \n",
    "        # if frontmatter_data['date_stats']:\n",
    "        #     stats = frontmatter_data['date_stats']\n",
    "        #     f.write(f\"> **Date range:** {stats['oldest'].strftime('%Y-%m-%d')} to {stats['newest'].strftime('%Y-%m-%d')}\\n\")\n",
    "        #     f.write(f\"> **Timespan:** {stats['timespan'].days} days\\n\")\n",
    "        #     f.write(f\"> **Average files per day:** {stats['files_per_day']:.2f}\\n\\n\")\n",
    "        \n",
    "        # Property Prevalence\n",
    "        # f.write(\"\\n# Resumo propriedades\\n\")\n",
    "        f.write(\"| Property | Files | Coverage |\\n\")\n",
    "        f.write(\"|----------|-------|----------|\\n\")\n",
    "        for prop, count in frontmatter_data['property_presence'].most_common(15):\n",
    "            coverage = (count / frontmatter_data['total_files']) * 100\n",
    "            f.write(f\"| `{prop}` | {count} | {coverage:.1f}% |\\n\")\n",
    "        \n",
    "        # Property Value Analysis\n",
    "        # f.write(\"\\n## Resumos por Propriedade\\n\")\n",
    "        # for prop, counter in frontmatter_data['property_stats'].items():\n",
    "        #     total = sum(counter.values())\n",
    "        #     unique = len(counter)\n",
    "        #     presence = frontmatter_data['property_presence'][prop]\n",
    "        #     percentage = presence / frontmatter_data['total_files']\n",
    "            \n",
    "        #     f.write(f\"\\n#### `{prop}`\\n\")\n",
    "        #     f.write(f\"- 📁 Present in: {presence} files ({percentage:.1%})\\n\")\n",
    "        #     f.write(f\"- 🧮 Unique values: {unique}\\n\")\n",
    "        \n",
    "        #     if unique <= 15:\n",
    "        #         f.write(\"  - 🔢 Values:\\n\")\n",
    "        #         for value, count in counter.most_common():\n",
    "        #             f.write(f\"    - `{value}`: {count} ({count/total:.1%})\\n\")\n",
    "        #     else:\n",
    "        #         f.write(\"  - 🔝 Top values:\\n\")\n",
    "        #         top_values = counter.most_common(5)\n",
    "        #         for value, count in top_values:\n",
    "        #             f.write(f\"    - `{value}`: {count} ({count/total:.1%})\\n\")\n",
    "        #         others = total - sum(count for _, count in top_values)\n",
    "        #         f.write(f\"    - `Others` ({unique - 5} values): {others} ({others/total:.1%})\\n\")\n",
    "\n",
    "        \n",
    "        f.write(\"\\n---\\n\\n\")\n",
    "        f.write(\"# 🗃️ **Pastas**\\n\\n\")\n",
    "        \n",
    "     \n",
    "        \n",
    "        # Listagem hierárquica das notas\n",
    "        # Listagem hierárquica das notas\n",
    "# Listagem hierárquica das notas\n",
    "        pastas_escritas = set()\n",
    "        \n",
    "        # Ícones por nível\n",
    "        icones_pasta = {\n",
    "            1: \"📁\",\n",
    "            2: \"📂\",\n",
    "            3: \"📘\",\n",
    "            4: \"📙\",\n",
    "            5: \"📗\",\n",
    "            6: \"📄\",\n",
    "        }\n",
    "        \n",
    "        for caminho_completo, notas in sorted(notas_organizadas.items()):\n",
    "            partes = caminho_completo.split(os.sep)\n",
    "            \n",
    "            caminho_acumulado = []\n",
    "            for i, parte in enumerate(partes):\n",
    "                caminho_acumulado.append(parte)\n",
    "                chave = os.sep.join(caminho_acumulado)\n",
    "                \n",
    "                if chave not in pastas_escritas:\n",
    "                    header_level = min(i + 1, 6)\n",
    "                    icone = icones_pasta.get(header_level, \"📦\")\n",
    "                    f.write(f\"{'#' * header_level} {icone} {parte}\\n\\n\")\n",
    "                    pastas_escritas.add(chave)\n",
    "        \n",
    "            for nome_nota, palavras in sorted(notas, key=lambda x: x[1], reverse=True):\n",
    "                f.write(f\"- 📄 [{nome_nota}] — {formatar_numero(palavras)} palavras\\n\")\n",
    "            \n",
    "            total_pasta = contagem_por_pasta[caminho_completo]\n",
    "            media_pasta = total_pasta / len(notas) if len(notas) > 0 else 0\n",
    "            f.write(f\"\\n**📊 Estatísticas da pasta**:\\n\")\n",
    "            f.write(f\"- Total: {formatar_numero(total_pasta)} palavras\\n\")\n",
    "            f.write(f\"- Média por nota: {formatar_numero(media_pasta)} palavras\\n\")\n",
    "            f.write(f\"- Número de notas: {formatar_numero(len(notas))}\\n\\n\")\n",
    "\n",
    "        \n",
    "        f.write(\"---\\n\")\n",
    "        f.write(f\"## 📊 Total geral: {formatar_numero(total_geral)} palavras em {formatar_numero(total_notas)} notas\\n\")\n",
    "        f.write(f\"## 📈 Média geral: {formatar_numero(total_geral/total_notas if total_notas > 0 else 0)} palavras por nota\\n\")\n",
    "\n",
    "# Caminhos\n",
    "caminho_da_pasta = r\"C:\\Users\\nonak\\Documents\\Thoughts\"\n",
    "caminho_arquivo_saida = os.path.join(caminho_da_pasta, r\"System\\index_notas.md\")\n",
    "\n",
    "# Execução\n",
    "notas_organizadas, contagem_por_pasta, total_palavras, total_notas, datas_modificacao = listar_notas_markdown_organizadas(caminho_da_pasta)\n",
    "frontmatter_data = analyze_frontmatter(caminho_da_pasta)\n",
    "salvar_em_markdown(notas_organizadas, contagem_por_pasta, total_palavras, total_notas, datas_modificacao, frontmatter_data, caminho_arquivo_saida)\n",
    "\n",
    "print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10896576-1646-46ec-bb35-bfdb65e93003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
